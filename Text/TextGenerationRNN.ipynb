{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# check the data\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\n"
     ]
    }
   ],
   "source": [
    "# check some text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# check unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "source": [
    "# Preprocess data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### vectorize text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# mapping from indices to chars\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# convert text data to integers\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  '\\n':   0,\n  ' ' :   1,\n  '!' :   2,\n  '$' :   3,\n  '&' :   4,\n  \"'\" :   5,\n  ',' :   6,\n  '-' :   7,\n  '.' :   8,\n  '3' :   9,\n  ':' :  10,\n  ';' :  11,\n  '?' :  12,\n  'A' :  13,\n  'B' :  14,\n  'C' :  15,\n  'D' :  16,\n  'E' :  17,\n  'F' :  18,\n  'G' :  19,\n  ...\n}\n"
     ]
    }
   ],
   "source": [
    "# check the character to index mapping\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# check mapping of the text\n",
    "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "source": [
    "### create training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F\ni\nr\ns\nt\n"
     ]
    }
   ],
   "source": [
    "# max sequence length\n",
    "seq_length = 100\n",
    "\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# create dataset\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# check dataset\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "# use batching to convert individual indices to a sequence\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "# check sequence\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sequence, shift one to create the label\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\nTarget data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# check the dataset\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "# check logic\n",
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "source": [
    "# Create model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each vocab is a character\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (64, None, 256)           16640     \n_________________________________________________________________\ngru (GRU)                    (64, None, 1024)          3938304   \n_________________________________________________________________\ndense (Dense)                (64, None, 65)            66625     \n=================================================================\nTotal params: 4,021,569\nTrainable params: 4,021,569\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# check output\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: \n \"your shame, by this unholy braggart,\\n'Fore your own eyes and ears?\\n\\nAll Conspirators:\\nLet him die fo\"\n\nNext Char Predictions: \n \"q?q$g RHbPPN$iODcLsTKHhfThBi 'fQJ!VbsPZnSGeQMQKpemcfQb-La!.N?TGzgYNk- ecvT. -nrIXm!X nMBY!z,qbL\\nHLEr\"\n"
     ]
    }
   ],
   "source": [
    "# test model on a sample\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=1).numpy()\n",
    "\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "source": [
    "# Train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\nscalar_loss:       4.1745925\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoints\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 2.6697\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 1.9589\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 1.6925\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 1.5427\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 1.4545\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 1.3943\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 1.3498\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 1.3106\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 1.2751\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 1.2423\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "source": [
    "# Generate text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (1, None, 256)            16640     \n_________________________________________________________________\ngru_1 (GRU)                  (1, None, 1024)           3938304   \n_________________________________________________________________\ndense_1 (Dense)              (1, None, 65)             66625     \n=================================================================\nTotal params: 4,021,569\nTrainable params: 4,021,569\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# the model only takes fixed batch size once built\n",
    "# if want to run model with different batch size, need to rebuild model and store weights\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # convert input string to integers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # low temperature for more predictable text, high vice versa\n",
    "    temperature = 1.0\n",
    "\n",
    "    # batch size = 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # use categorical distribution to predict character\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # pass predicted character and previous hidden state as next input to the model\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROMEO: 'tis there.\nGo tell you, prompts it.\n\nKING HENRY VI:\nI will, indeed to say; I fear\nWe make road heart; and of his mouth'd dram not.\n\nTYNREL:\nHe doth short them would here you in the orderves.\nThe harl: thou camer with in that Inemarity\nHis friends are constant to cry to Rome, that justice, or how, or foe\nThan appear the rest? I will not hear me now,\nAnd I fetch the boldly Belunkin'd. Now I can leave.\n\nEDWARD:\nNothand seems.\nWhat worthy truck out on a time-oast,\nOf that hour for Rome are tor mer licks.\nFum to their in his admits of a tedious.\nA king of kinghood for you to appear,' quarce are humour\nwithin your heart\nPrick'd once.\n\nSICINIUS:\nGood mock-grace.\n\nLord:\nHis vaultiel the misery remors.\n\nHORTENSIO:\nYes, sir; well, we'll not hell.\n\nNORFOLK:\nHer certain!\n\nSICINIUS:\nNow, fall, so too; shall see thee, I say!\nTake thou to jest; treading him, or seeming kind his graces\nOf that heartily were lodgiest\nMaster of all this world's pallan shall Look on her.\n\nPETRUCHIO:\nThey'll we would be \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u'ROMEO: '))"
   ]
  },
  {
   "source": [
    "# Custom training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size=len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(input, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(input)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(target, predictions, from_logits=True))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Batch 0 Loss 4.174837589263916\n",
      "Epoch 1 Batch 100 Loss 2.371006965637207\n",
      "Epoch 1 Loss 2.1392\n",
      "Time taken for 1 epoch 18.51416540145874 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.144247055053711\n",
      "Epoch 2 Batch 100 Loss 1.8878687620162964\n",
      "Epoch 2 Loss 1.7631\n",
      "Time taken for 1 epoch 17.27969002723694 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.7810107469558716\n",
      "Epoch 3 Batch 100 Loss 1.6371573209762573\n",
      "Epoch 3 Loss 1.5847\n",
      "Time taken for 1 epoch 17.39772319793701 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.5576701164245605\n",
      "Epoch 4 Batch 100 Loss 1.5388844013214111\n",
      "Epoch 4 Loss 1.5136\n",
      "Time taken for 1 epoch 17.336246252059937 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.4515957832336426\n",
      "Epoch 5 Batch 100 Loss 1.4678282737731934\n",
      "Epoch 5 Loss 1.4833\n",
      "Time taken for 1 epoch 17.685662508010864 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.3714306354522705\n",
      "Epoch 6 Batch 100 Loss 1.455464482307434\n",
      "Epoch 6 Loss 1.3793\n",
      "Time taken for 1 epoch 17.329514026641846 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.313724160194397\n",
      "Epoch 7 Batch 100 Loss 1.32773756980896\n",
      "Epoch 7 Loss 1.3541\n",
      "Time taken for 1 epoch 17.55643630027771 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.2929421663284302\n",
      "Epoch 8 Batch 100 Loss 1.3553394079208374\n",
      "Epoch 8 Loss 1.3152\n",
      "Time taken for 1 epoch 17.584121227264404 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.2221381664276123\n",
      "Epoch 9 Batch 100 Loss 1.3003318309783936\n",
      "Epoch 9 Loss 1.3034\n",
      "Time taken for 1 epoch 17.538541078567505 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.2256475687026978\n",
      "Epoch 10 Batch 100 Loss 1.263058066368103\n",
      "Epoch 10 Loss 1.2515\n",
      "Time taken for 1 epoch 18.126379251480103 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # reset hidden state at every epoch\n",
    "    model.reset_states()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        loss = train_step(inp, target)\n",
    "\n",
    "        if batch_n % 100 == 0:\n",
    "            template = 'Epoch {} Batch {} Loss {}'\n",
    "            print(template.format(epoch + 1, batch_n, loss))\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}