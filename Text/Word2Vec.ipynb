{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "source": [
    "# One sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The wide road shimmered in the hot sun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# tokenize it\n",
    "tokens = list(sentence.lower().split())\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
     ]
    }
   ],
   "source": [
    "# create a vocab to save mappings from tokens to integers\n",
    "vocab, index = {}, 1 # start indexing from 1\n",
    "vocab['<pad>'] = 0 # add a padding token\n",
    "for token in tokens:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
     ]
    }
   ],
   "source": [
    "# create an inverse vocab to save mappings from integers to tokens\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# vectorize the sentence using vocab\n",
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)"
   ]
  },
  {
   "source": [
    "### generate skip-grams"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    example_sequence,\n",
    "    vocabulary_size=vocab_size,\n",
    "    window_size=window_size,\n",
    "    negative_samples=0 # no negative samples for now, will use in the next section\n",
    ")\n",
    "len(positive_skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 2): (the, wide)\n(4, 5): (shimmered, in)\n(2, 1): (wide, the)\n(1, 4): (the, shimmered)\n(4, 3): (shimmered, road)\n"
     ]
    }
   ],
   "source": [
    "# check some skip-grams\n",
    "for target, context in positive_skip_grams[:5]:\n",
    "    print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ]
  },
  {
   "source": [
    "### negative skip-grams"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n['wide', 'the', 'shimmered', 'road']\n"
     ]
    }
   ],
   "source": [
    "# also need to generate negative samples by randomly sampling words in the sentence\n",
    "# for each target word, exclude its true context word from being sampled\n",
    "\n",
    "# get target and context words for one positive skip-gram\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# number of negative samples per positive context, [5, 20] is best for small datasets, while [2, 5] is for larget datasets\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype='int64'), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class, # class that should be sampled as positive\n",
    "    num_true=1, # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns, # number of negative context words per sample\n",
    "    unique=True, # all negative samples should be unique\n",
    "    range_max=vocab_size, # pick samples from [0, vocab_size]\n",
    "    seed=seed,\n",
    "    name='negative_sampling'\n",
    ")\n",
    "\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "source": [
    "### construct one training example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a dimnesion so can be concatenated\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "# concat pos and neg samples\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# label first context word as positive, and rest negative\n",
    "label = tf.constant([1] + [0] * num_ns, dtype='int64')\n",
    "\n",
    "# reshape target to (1, ) and context and label to (num_ns+1, )\n",
    "target = tf.squeeze(target_word)\n",
    "context = tf.squeeze(context)\n",
    "label = tf.squeeze(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target_index    : 1\ntarget_word     : the\ncontext_indices : [2 2 1 4 3]\ncontext_words   : ['wide', 'wide', 'the', 'shimmered', 'road']\nlabel           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# check the sample\n",
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target  : tf.Tensor(1, shape=(), dtype=int32)\ncontext : tf.Tensor([2 2 1 4 3], shape=(5,), dtype=int64)\nlabel   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# a tuple of (target, context, label) tensor is one training example\n",
    "print(f\"target  :\", target)\n",
    "print(f\"context :\", context )\n",
    "print(f\"label   :\", label )"
   ]
  },
  {
   "source": [
    "# Compile all steps in one function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "# a sampling table can be used to not sampling frequent words (e.g. the) too much\n",
    "# it is the probability of sampling the ith most common word in the dataset\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate skip-grams with negative sampling for a list of sequences (int-encoded sentences)\n",
    "# based on window size, number of negative samples, and vocab size\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    # elements of each training sample are appended to these lists\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # build sampling table\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        # generate postive skip-grams\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0\n",
    "        )\n",
    "\n",
    "        # iterate the positive skip-grams to generate negative training samples\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype='int64'), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name='negative_sampling'\n",
    "            )\n",
    "\n",
    "            # build context and label vectors for one target word\n",
    "            negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0] * num_ns, dtype='int64')\n",
    "\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context),\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "source": [
    "# Prepare training data for Word2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### get data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "# check some data\n",
    "with open(path_to_file) as f:\n",
    "    lines = f.read().splitlines()\n",
    "for line in lines[:20]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty lines and build dataset\n",
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "source": [
    "### vectorize sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom standardization function to lowercase and remove punctuation\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "# vocab size and number of words in a sequence\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# use TextVectorization layer to normalize, split, and map strings to integers\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length # pad all samples to same length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt on the training set to create vocabulary\n",
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
     ]
    }
   ],
   "source": [
    "# check the vocabulary\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "# order is based on frequency\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the dataset\n",
    "\n",
    "def vectorize_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return tf.squeeze(vectorize_layer(text))\n",
    "\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "source": [
    "### obtain sequences from dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32777"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# flatten the dataset into a list of sentence vector sequences\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "# check some sequences\n",
    "for seq in sequences[:5]:\n",
    "    print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "source": [
    "### generate training example from sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32777/32777 [00:27<00:00, 1200.76it/s]64866 64866 64866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "print(len(targets), len(contexts), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "# configure dataset\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# create dataset of (target_word, context_word), (label)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "source": [
    "# Build model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "\n",
    "        # target embedding layer that looks up embedding of a word when it is a target word\n",
    "        self.target_embedding = Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            input_length=1,\n",
    "            name='w2v_embedding'\n",
    "        )\n",
    "        # context embedding layer that looks up embedding of a word when it is a context word\n",
    "        self.context_embedding = Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            input_length=num_ns+1\n",
    "        )\n",
    "        # dot product to obtain predictions for labels\n",
    "        self.dots = Dot(axes=(3,2))\n",
    "        # flatten to logits\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        we = self.target_embedding(target)\n",
    "        ce = self.context_embedding(context)\n",
    "        dots = self.dots([ce, we])\n",
    "        return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for tensorboard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      " 1/63 [..............................] - ETA: 0s - loss: 1.6096 - accuracy: 0.1992WARNING:tensorflow:From c:\\SDK\\venv\\TensorflowEnv\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/63 [..............................] - ETA: 2s - loss: 1.6097 - accuracy: 0.1934WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_train_batch_end` time: 0.0609s). Check your callbacks.\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.6083 - accuracy: 0.2297\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.5893 - accuracy: 0.5532\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.5426 - accuracy: 0.6016\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.4608 - accuracy: 0.5770\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.3628 - accuracy: 0.5833\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.2653 - accuracy: 0.6106\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.1739 - accuracy: 0.6448\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.0892 - accuracy: 0.6797\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.0107 - accuracy: 0.7121\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.9381 - accuracy: 0.7400\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.8710 - accuracy: 0.7648\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.8092 - accuracy: 0.7866\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.7523 - accuracy: 0.8069\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.7002 - accuracy: 0.8231\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.6525 - accuracy: 0.8382\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.6089 - accuracy: 0.8515\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.5692 - accuracy: 0.8642\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.5330 - accuracy: 0.8748\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.5001 - accuracy: 0.8844\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.4701 - accuracy: 0.8933\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1862192d6a0>"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "source": [
    "# Save word embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding weights\n",
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "\n",
    "# get vocab\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights to disk\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue # skip 0, it is padding\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    out_m.write(word + '\\n')\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}