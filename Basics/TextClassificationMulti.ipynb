{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599833103271",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"stack_overflow_16k.tar.gz\", url, untar=True, cache_dir='.', cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'stackoverflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['README.md', 'test', 'train']"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# check the files\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 8000 files belonging to 4 classes.\nUsing 6400 files for training.\nFound 8000 files belonging to 4 classes.\nUsing 1600 files for validation.\nFound 8000 files belonging to 4 classes.\n"
    }
   ],
   "source": [
    "# create raw tf.data.Dataset\n",
    "\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "# raw_train_ds will be a tf.data.Dataset object\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'stackoverflow/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2, # also split some validation set, created later\n",
    "    subset='training', # specify this invoke is for training set\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# create validation set\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'stackoverflow/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation', # specify this invoke is for validation set\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# create test set\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'stackoverflow/test',\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a TextVectorization layer standardizes, tokenizes, and vectorizes text data\n",
    "\n",
    "max_features = 5000\n",
    "sequence_length = 500 # only keep 500 words per sample\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a text only dataset (without labels), then adapt\n",
    "# adapt fits the state of the preprocessing layer to the dataset, building an index of strings to integers (vectorization)\n",
    "# only use adapt on train data\n",
    "train_text = raw_train_ds.map(lambda x, y: x)  # only keep the text\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess data\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1) # expand dimension at end\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add TextVectorization layer to train, val, test datasets\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache() and prefetch() improves efficiency of datasets\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "# Embedding layer converts word-index into embedding vectors, adds a dimension, (batch, sequence, embedding)\n",
    "# GlobalAveragePooling1D layer averages the sequence dimension, returning a fixed length vector for each sample, this is to handle inputs of different length\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(max_features+1, embedding_dim),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(4) # four classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model with cost function and optimizer\n",
    "# use SparseCategoricalCrossentropy because multiclass classification\n",
    "model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/5\n200/200 [==============================] - 5s 24ms/step - loss: 1.3729 - accuracy: 0.3220 - val_loss: 1.3555 - val_accuracy: 0.4363\nEpoch 2/5\n200/200 [==============================] - 2s 10ms/step - loss: 1.3183 - accuracy: 0.4638 - val_loss: 1.2784 - val_accuracy: 0.5294\nEpoch 3/5\n200/200 [==============================] - 2s 10ms/step - loss: 1.2162 - accuracy: 0.5728 - val_loss: 1.1590 - val_accuracy: 0.6637\nEpoch 4/5\n200/200 [==============================] - 2s 10ms/step - loss: 1.0933 - accuracy: 0.6514 - val_loss: 1.0389 - val_accuracy: 0.7119\nEpoch 5/5\n200/200 [==============================] - 2s 10ms/step - loss: 0.9798 - accuracy: 0.7022 - val_loss: 0.9366 - val_accuracy: 0.7400\n"
    }
   ],
   "source": [
    "epochs = 5\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "250/250 [==============================] - 4s 17ms/step - loss: 0.9630 - accuracy: 0.7096\nLoss:  0.9630272388458252\nAccuracy:  0.7096250057220459\n"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}